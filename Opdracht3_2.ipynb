{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP231j1nP52wHk1d+q7E/Sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaD01/Deep-learning-/blob/Henry/Opdracht3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yZmOVDC_A-js",
        "outputId": "c6ac188e-39c6-4004-e0da-fc97d26febed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import datasets\n",
        "from datasets import load_dataset, Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "_QmajFrMhMcq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXjsu9V7f5Sk",
        "outputId": "a7ad7c45-c701-4ec3-97b1-13520eb046f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "8qReyY4_Ex0d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfFolder\n",
        "from huggingface_hub import whoami\n",
        "\n",
        "os.environ['HF_TOKEN'] = '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'\n",
        "\n",
        "# Load the token from the environment variable\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "# Authenticate with Hugging Face using the token\n",
        "\n",
        "HfFolder.save_token(hf_token)\n",
        "\n",
        "# Verify the authentication (optional)\n",
        "user = whoami()\n",
        "print(f\"Authenticated as: {user['name']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-Csf_UsC4zc",
        "outputId": "4cd2c5e6-2deb-4f28-f7af-c4626d9c5e75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated as: ThugWithin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('tomytjandra/h-and-m-fashion-caption-12k')"
      ],
      "metadata": {
        "id": "byGWDvZ7BGhA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn6HPfI5IyWz",
        "outputId": "9efadb1a-a04b-406d-e153-a079569908c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y10LWcFSTACH",
        "outputId": "865dd6b8-d301-4806-af85-508b8a7bc79c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'image'],\n",
              "    num_rows: 12437\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def dataset_to_dataframe(dataset, chunk_size=1000, output_file='output.h5'):\n",
        "\n",
        "#     # Initialize an empty DataFrame to store the results\n",
        "#     dataframe = pd.DataFrame()\n",
        "\n",
        "#     # Open an HDF5 file to store data incrementally\n",
        "#     with pd.HDFStore(output_file, mode='w') as store:\n",
        "\n",
        "#         # Determine the total number of chunks\n",
        "#         num_chunks = len(dataset) // chunk_size + (1 if len(dataset) % chunk_size != 0 else 0)\n",
        "\n",
        "#         for chunk_idx in range(num_chunks):\n",
        "#             caps = []\n",
        "#             im_arrays = []\n",
        "#             heights = []\n",
        "#             widths = []\n",
        "\n",
        "#             start_idx = chunk_idx * chunk_size\n",
        "#             end_idx = min((chunk_idx + 1) * chunk_size, len(dataset))\n",
        "\n",
        "#             texts = dataset['text'][start_idx:end_idx]\n",
        "#             images = dataset['image'][start_idx:end_idx]\n",
        "\n",
        "#             for i in range(len(texts)):\n",
        "#                 caps.append(texts[i])\n",
        "#                 im_arrays.append(np.array(images[i]))\n",
        "#                 heights.append(images[i].height)\n",
        "#                 widths.append(images[i].width)\n",
        "\n",
        "#             chunk_df = pd.DataFrame({\n",
        "#                 'caption': caps,\n",
        "#                 'image_array': im_arrays,\n",
        "#                 'height': heights,\n",
        "#                 'width': widths\n",
        "#             })\n",
        "\n",
        "#             # Append the chunk to the HDF5 file\n",
        "#             store.append('data', chunk_df, format='table', data_columns=True)\n",
        "\n",
        "#     print(f\"Data saved to {output_file}\")"
      ],
      "metadata": {
        "id": "qTFx_vKHrIAl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_to_dataframe(dataset['train'], chunk_size=BATCH_SIZE, output_file='dataframe.h5')"
      ],
      "metadata": {
        "id": "hwW-oR8vrJMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_to_dataframe(dataset):\n",
        "\n",
        "  caps = []\n",
        "  im_arrays = []\n",
        "  heights = []\n",
        "  widths = []\n",
        "\n",
        "  texts = dataset['text']\n",
        "  images = dataset['image']\n",
        "  for i in range(len(dataset)):\n",
        "    caps.append(texts[i])\n",
        "    im_arrays.append(np.array(images[i]))\n",
        "    heights.append(images[i].height)\n",
        "    widths.append(images[i].width)\n",
        "\n",
        "  dataframe = pd.DataFrame({\n",
        "                'caption': caps,\n",
        "                'image_array': im_arrays,\n",
        "                'height': heights,\n",
        "                'width': widths})\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "def clean_and_tokenize(caption):\n",
        "    # Convert to lowercase\n",
        "    caption = caption.lower()\n",
        "    # Remove punctuation\n",
        "    caption = re.sub(r'[^\\w\\s]', '', caption)\n",
        "    # Tokenize (split into words)\n",
        "    words = caption.split()\n",
        "    return words\n",
        "\n",
        "def clean_caption(caption):\n",
        "    # Convert to lowercase\n",
        "    caption = caption.lower()\n",
        "    # Remove special characters except spaces\n",
        "    caption = re.sub(r'[^a-zA-Z0-9\\s]', '', caption)\n",
        "    # Remove extra whitespaces\n",
        "    caption = re.sub(r'\\s+', ' ', caption).strip()\n",
        "    return caption\n",
        "\n",
        "def text_features(dataframe, text_col):\n",
        "  dataframe['text_length'] = [len(text) for text in dataframe[text_col]]\n",
        "  dataframe['tokens'] = dataframe[text_col].apply(clean_and_tokenize)\n",
        "  dataframe['count_tokens'] = [len(tokens) for tokens in dataframe['tokens']]\n",
        "\n",
        "  dataframe['cleaned_text'] = dataframe[text_col].apply(clean_caption)\n",
        "  dataframe['cleaned_text_length'] = [len(text) for text in dataframe['cleaned_text']]\n",
        "  dataframe['cleaned_tokens'] = dataframe['cleaned_text'].apply(clean_and_tokenize)\n",
        "  dataframe['cleaned_count_tokens'] = [len(tokens) for tokens in dataframe['cleaned_tokens']]\n",
        "\n",
        "  dataframe['diff_text_length'] = dataframe['text_length'] - dataframe['cleaned_text_length']\n",
        "  dataframe['diff_count_tokens'] = dataframe['count_tokens'] - dataframe['cleaned_count_tokens']"
      ],
      "metadata": {
        "id": "tHA5S1PImews"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = dataset_to_dataframe(dataset['train'])\n",
        "dataframe = text_features(dataframe, 'caption')\n",
        "display(dataframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "grCQC2FDm_89",
        "outputId": "19351063-a1b5-4680-f3a0-3ca4c0e7dbba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'_PrefetchDataset' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ea8a53499969>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caption'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e2a22fd8c89d>\u001b[0m in \u001b[0;36mdataset_to_dataframe\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mwidths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '_PrefetchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/datasets/use_with_tensorflow"
      ],
      "metadata": {
        "id": "0fNT1XLdNlDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = dataset['train'].column_names\n",
        "\n",
        "tf_ds = dataset['train'].to_tf_dataset(\n",
        "            columns=cols,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True\n",
        "            )\n",
        "\n",
        "tf_ds = tf_ds.cache().prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "_F-AAUkcLW-E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(tf_ds)\n",
        "validation_size = int(0.2 * dataset_size)\n",
        "\n",
        "val_ds = tf_ds.take(validation_size)\n",
        "train_ds = tf_ds.skip(validation_size)"
      ],
      "metadata": {
        "id": "KsazvpNnM_NH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display(dataframe.info(), dataframe.describe())"
      ],
      "metadata": {
        "id": "n184AEgjZelO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# # Distribution of heights and widths\n",
        "# fig, axes = plt.subplots(2, figsize=(10,10))\n",
        "# axes[0].xaxis.set_major_formatter(ScalarFormatter())\n",
        "# axes[1].xaxis.set_major_formatter(ScalarFormatter())\n",
        "# dataframe['height'].plot(kind='hist', ax=axes[0], title='height')\n",
        "# dataframe['width'].plot(kind='hist', ax=axes[1], title='width')\n",
        "# plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ehiIqNleaCPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# # Check if the images are RGB\n",
        "# for url in dataframe['src'][:5]:\n",
        "#   r = requests.get(url)\n",
        "#   img = Image.open(BytesIO(r.content))\n",
        "#   plt.imshow(img)\n",
        "#   plt.axis('off')\n",
        "#   plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sgPgZoYMaDBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def clean_and_tokenize(caption):\n",
        "#     # Convert to lowercase\n",
        "#     caption = caption.lower()\n",
        "#     # Remove punctuation\n",
        "#     caption = re.sub(r'[^\\w\\s]', '', caption)\n",
        "#     # Tokenize (split into words)\n",
        "#     words = caption.split()\n",
        "#     return words"
      ],
      "metadata": {
        "id": "2C8bhbNF8vei"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe['text_length'] = [len(text) for text in dataframe['text']]\n",
        "# dataframe['tokens'] = dataframe['text'].apply(clean_and_tokenize)\n",
        "# dataframe['count_tokens'] = [len(tokens) for tokens in dataframe['tokens']]"
      ],
      "metadata": {
        "id": "wfoH879-c95J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # top words excluded stopwords\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# all_tokens = [word for tokens in dataframe['tokens'] for word in tokens]\n",
        "# all_tokens = [word for word in all_tokens if word not in stop_words]\n",
        "# top_25 = Counter(all_tokens).most_common(25)\n",
        "# print(top_25)"
      ],
      "metadata": {
        "id": "zGq59xgmen9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_25))\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Mnuzxp5geopM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_words = set(all_tokens)\n",
        "# vocabulary_size = len(unique_words)\n",
        "# print(vocabulary_size)"
      ],
      "metadata": {
        "id": "5qH_attvhD-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def clean_caption(caption):\n",
        "#     # Convert to lowercase\n",
        "#     caption = caption.lower()\n",
        "#     # Remove special characters except spaces\n",
        "#     caption = re.sub(r'[^a-zA-Z0-9\\s]', '', caption)\n",
        "#     # Remove extra whitespaces\n",
        "#     caption = re.sub(r'\\s+', ' ', caption).strip()\n",
        "#     return caption\n",
        "\n",
        "# dataframe['cleaned_text'] = dataframe['text'].apply(clean_caption)"
      ],
      "metadata": {
        "id": "sfMvGY7cheE6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe['cleaned_text_length'] = [len(text) for text in dataframe['cleaned_text']]\n",
        "# dataframe['cleaned_tokens'] = dataframe['cleaned_text'].apply(clean_and_tokenize)\n",
        "# dataframe['cleaned_count_tokens'] = [len(tokens) for tokens in dataframe['cleaned_tokens']]"
      ],
      "metadata": {
        "id": "rKzz5TRlh8RD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe['diff_text_length'] = dataframe['text_length'] - dataframe['cleaned_text_length']\n",
        "# dataframe['diff_count_tokens'] = dataframe['count_tokens'] - dataframe['cleaned_count_tokens']"
      ],
      "metadata": {
        "id": "lnj0xeqHiQlN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe.describe()"
      ],
      "metadata": {
        "id": "NorBVH5MiRSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chatgpt.com/share/612bf8e2-1eec-4402-b332-2ebb0da2154c"
      ],
      "metadata": {
        "id": "r_sMtzR7_O_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "29B5iYm7eMZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://chatgpt.com/share/ae24adc4-f7a9-4ca4-9519-066c91814b25\n",
        "\n",
        "# # Define the directory to save images\n",
        "# image_dir = 'images'\n",
        "# cap_dir = 'captions'\n",
        "# os.makedirs(image_dir, exist_ok=True)\n",
        "# os.makedirs(cap_dir, exist_ok=True)\n",
        "\n",
        "# # Function to download an image\n",
        "# def download_image(url, save_path):\n",
        "#     try:\n",
        "#         response = requests.get(url, stream=True)\n",
        "#         if response.status_code == 200:\n",
        "#             with open(save_path, 'wb') as f:\n",
        "#                 for chunk in response.iter_content(1024):\n",
        "#                     f.write(chunk)\n",
        "#         else:\n",
        "#             print(f\"Failed to download {url}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error downloading {url}: {e}\")\n",
        "\n",
        "# # Function to save a caption\n",
        "# def save_caption(caption, save_path):\n",
        "#     try:\n",
        "#         with open(save_path, 'w') as f:\n",
        "#             f.write(caption)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error saving caption: {e}\")\n",
        "\n",
        "# # Download each image\n",
        "# for idx, row in tqdm(dataframe[['text','src']].iterrows(), total=dataframe[['text','src']].shape[0]):\n",
        "#     url = row['src']\n",
        "#     cap = row['text']\n",
        "#     image_path = os.path.join(image_dir, f'image_{idx}.jpg')\n",
        "#     caption_path = os.path.join(cap_dir, f'caption_{idx}.txt')\n",
        "\n",
        "#     download_image(url, image_path)\n",
        "#     save_caption(cap, caption_path)\n",
        "\n",
        "# print(\"Download completed!\")\n"
      ],
      "metadata": {
        "id": "H20bhnI5i-RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE_PATH = image_dir\n",
        "\n",
        "# IMAGE_HEIGHT = 1750\n",
        "# IMAGE_WIDTH = 1166\n",
        "# IMAGE_SIZE = (IMAGE_HEIGHT,IMAGE_WIDTH)\n",
        "\n",
        "# CAPTION_PATH = cap_dir\n",
        "\n",
        "# VOCAB_SIZE = vocabulary_size\n",
        "\n",
        "# SEQ_LENGTH = 25\n",
        "\n",
        "# EMBED_DIM = 512\n",
        "\n",
        "# FF_DIM = 512\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "# EPOCHS = 25\n",
        "# AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "4ouDRbGPcdbE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBykYPg1fbzF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}